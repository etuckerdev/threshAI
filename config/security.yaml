detection_thresholds:
  injection: 0.89
  data_exfil: 0.93
  model_poison: 0.97
  quantum_anomaly: 0.95

approved_models:
  - cas/ministral-8b-instruct-2410_q4km
  - withsecure/llama3-8b:backup

quantum_parameters:
  lattice:
    dimension: 512
    modulus: 12289
    secret_distribution: "uniform"
  temporal_smearing:
    window_size: 1000
    max_variance: 0.1
  memory_protection:
    quantum_safe: true
    post_quantum_allocator: true

fallback_regex:
  sqli: "(?i)(drop\s+\w+|;\s*--|union\s+select)"
  xss: "(<script|alert\(|document\.cookie)"

# Brutal Mode Quantization Presets
brutal_presets:
  1:
    quant: "Q2_K"
    vram_limit: "4G"
    description: "Gremlin Mode - 2-bit chaos"
    chaos_multiplier: 0.3
    
  2: 
    quant: "Q4_K_M"
    vram_limit: "6G"
    description: "Standard Brutalization"
    chaos_multiplier: 1.0
    
  3:
    quant: "Q6_K"
    vram_limit: "8G"
    description: "Maximum Chaos"
    chaos_multiplier: 2.5
    unstable: true

validation:
  - name: "Model Verification"
    cmd: "ollama list | grep '8b-{quant}'"
    
  - name: "VRAM Enforcement"
    cmd: "nvidia-smi --query-gpu=memory.used --format=noheader,nounits"
    max: "${vram_limit}"

monitoring:
  chaos_metrics:
    - "output_coherence_score"
    - "quantum_entanglement_factor"
    - "tiktok_slang_ratio"